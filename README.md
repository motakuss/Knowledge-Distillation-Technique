# Knowledge-Distillation-Technique

## Papaers

* [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531), Hinton, 2015
* [FitNets: Hint for Thin Deep Nets](https://arxiv.org/abs/1412.6550), Romero, 2015
* [Improved Knowledge Distillation via Teacher Assistant](https://arxiv.org/abs/1902.03393), Mirzadeh, 2019
* [Multi-Stage Model Compression using Teacher Assistant and Distillation with Hint-Based Training](https://ieeexplore.ieee.org/document/9767229), Morikawa, 2022
* CNN Model Compression by Merit-based Distillation, Morikawa, 2023

## setup
```
python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```






